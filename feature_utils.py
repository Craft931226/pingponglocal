import csv
import logging
import math
from pathlib import Path
import numpy as np
from scipy.fft import rfft
import scipy.stats
import pandas as pd
from sklearn.metrics import roc_auc_score
logging.basicConfig(
    filename="log.txt",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
EXPECTED_DIM = 131  # Expected dimension of the feature vector

def aggregate_group_prob(proba_mat: np.ndarray, group_size: int = 27) -> np.ndarray:
    if proba_mat.size == 0:
        return np.array([[0]])
    if len(proba_mat) < group_size:
        return np.array([np.mean(proba_mat, axis=0)])
    num_groups = len(proba_mat) // group_size
    proba_mat = proba_mat[:num_groups * group_size]
    proba_mat = proba_mat.reshape(num_groups, group_size, -1)
    return proba_mat.mean(axis=1)

    
def FFT_data(input_data, swinging_times):   
    txtlength = swinging_times[-1] - swinging_times[0]
    a_mean = [0] * txtlength
    g_mean = [0] * txtlength
       
    for num in range(len(swinging_times)-1):
        a = []
        g = []
        for swing in range(swinging_times[num], swinging_times[num+1]):
            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))
            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))
        a_mean[num] = (sum(a) / len(a))
        g_mean[num] = (sum(g) / len(g))
    
    return a_mean, g_mean

def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):
    # Convert input data to numpy array
    arr = np.array(input_data)
    
    if swinging_times == 0:  # Handle case where there are no swings
        swinging_times = 1
        
    # Calculate frequency domain features
    cut = int(n_fft / swinging_times)
    idx_start = cut * swinging_now
    idx_end = min(cut * (swinging_now + 1), len(a_fft))  # Ensure we don't go out of bounds
    
    # Rest of the function remains the same...
    # Calculate acceleration and gyroscope vectors
    a_vec = np.sqrt(np.sum(arr[:, :3]**2, axis=1))
    g_vec = np.sqrt(np.sum(arr[:, 3:6]**2, axis=1))
    
    a_stats = [a_vec.max(), a_vec.mean(), a_vec.min()]
    g_stats = [g_vec.max(), g_vec.mean(), g_vec.min()]
    
    # Calculate kurtosis and skewness
    a_centered = a_vec - a_vec.mean()
    g_centered = g_vec - g_vec.mean()
    
    a_moments = {
        'skew': np.mean(a_centered**3) / (np.std(a_centered)**3),
        'kurt': np.mean(a_centered**4) / (np.var(a_centered)**2)
    }
    g_moments = {
        'skew': np.mean(g_centered**3) / (np.std(g_centered)**3),
        'kurt': np.mean(g_centered**4) / (np.var(g_centered)**2)
    }
    
    # Get FFT slices safely
    a_fft_slice = a_fft[idx_start:idx_end]
    g_fft_slice = g_fft[idx_start:idx_end]
    a_fft_imag_slice = a_fft_imag[idx_start:idx_end]
    g_fft_imag_slice = g_fft_imag[idx_start:idx_end]
    
    # Handle empty slices
    if len(a_fft_slice) == 0:
        a_fft_slice = np.array([0])
        g_fft_slice = np.array([0])
        a_fft_imag_slice = np.array([0])
        g_fft_imag_slice = np.array([0])
    
    # Calculate PSD using vectorized operations
    a_psd = np.power(a_fft_slice, 2) + np.power(a_fft_imag_slice, 2)
    g_psd = np.power(g_fft_slice, 2) + np.power(g_fft_imag_slice, 2)
    
    # Calculate entropy with safety checks
    e1 = np.sqrt(a_psd)
    e3 = np.sqrt(g_psd)
    e2 = np.sum(e1) + 1e-10  # Avoid division by zero
    e4 = np.sum(e3) + 1e-10
    
    p_a = e1 / e2
    p_g = e3 / e4
    entropy_a = np.sum(p_a * np.log(p_a + 1e-10)) / max(cut, 1)
    entropy_g = np.sum(p_g * np.log(p_g + 1e-10)) / max(cut, 1)
    
    # Calculate basic statistics
    mean = np.mean(arr, axis=0)
    std = np.std(arr, axis=0)
    rms = np.sqrt(np.mean(arr**2, axis=0))
    
    # Combine all features
    output = np.concatenate([
        mean, std, rms,
        a_stats, g_stats,
        [np.mean(a_fft_slice), np.mean(g_fft_slice)],
        [np.mean(a_psd), np.mean(g_psd)],
        [a_moments['kurt'], g_moments['kurt']],
        [a_moments['skew'], g_moments['skew']],
        [entropy_a, entropy_g]
    ]).tolist()
    
    writer.writerow(output)

def extract_features(df):
    features = []
    
    # é€Ÿåº¦ç›¸é—œç‰¹å¾µ
    speed_cols = ['Ax', 'Ay', 'Az']
    speeds = df[speed_cols].values
    
    # åŸºç¤çµ±è¨ˆç‰¹å¾µ
    features.extend([
        np.mean(speeds, axis=0),
        np.std(speeds, axis=0),
        np.max(speeds, axis=0),
        np.min(speeds, axis=0),
        np.percentile(speeds, 25, axis=0),
        np.percentile(speeds, 75, axis=0),
        scipy.stats.skew(speeds, axis=0),
        scipy.stats.kurtosis(speeds, axis=0)
    ])
    # --- Î” / Î”Â² ç‰¹å¾µï¼šæ•æ‰åŠ é€Ÿåº¦è®ŠåŒ–è¶¨å‹¢ ---
    delta1 = np.diff(speeds, axis=0, n=1)   # ä¸€éšå·®åˆ†
    delta2 = np.diff(speeds, axis=0, n=2)   # äºŒéšå·®åˆ†
    if delta1.size == 0:
        delta1 = np.zeros((1, 3))
    if delta2.size == 0:
        delta2 = np.zeros((1, 3))
    features.extend([
        np.mean(delta1, axis=0), np.std(delta1, axis=0),
        np.mean(delta2, axis=0), np.std(delta2, axis=0),
    ])
    # é€Ÿåº¦äº¤äº’ç‰¹å¾µ
    speed_means = np.mean(speeds, axis=0)
    features.extend([
        speed_means[0] * speed_means[1],  # x * y
        speed_means[1] * speed_means[2],  # y * z
        speed_means[0] * speed_means[2]   # x * z
    ])
    
    # åŠ é€Ÿåº¦ç‰¹å¾µ
    acc = np.diff(speeds, axis=0)
    features.extend([
        np.mean(acc, axis=0),
        np.std(acc, axis=0),
        np.max(acc, axis=0),
        np.min(acc, axis=0)
    ])
    
    # ä½ç½®ç›¸é—œç‰¹å¾µ
    pos_cols   = ['Gx', 'Gy', 'Gz']
    positions = df[pos_cols].values
    
    # ä½ç½®çµ±è¨ˆç‰¹å¾µ
    features.extend([
        np.mean(positions, axis=0),
        np.std(positions, axis=0),
        np.max(positions, axis=0),
        np.min(positions, axis=0),
        np.percentile(positions, 25, axis=0),
        np.percentile(positions, 75, axis=0)
    ])
    
    # ä½ç½®è®ŠåŒ–ç‰¹å¾µ
    pos_diff = np.diff(positions, axis=0)
    features.extend([
        np.mean(pos_diff, axis=0),
        np.std(pos_diff, axis=0)
    ])
    
    # æ™‚åºç‰¹å¾µ
    for col in speed_cols + pos_cols:
        #
        ts = df[col].values
        # FFTç‰¹å¾µ
        n = len(ts)
        window = np.hanning(n)                    # 1. å»ºç«‹çª—å‡½æ•¸
        ts_windowed = ts * window                 # 2. ä¹˜ä¸Šçª—
        n_fft = 1 << (n - 1).bit_length()         # 3. è¨ˆç®—ä¸‹å€‹ 2 çš„å†ªæ¬¡
        ts_padded = np.pad(ts_windowed,           # 4. ä»¥é›¶è£œé½Š
                        (0, n_fft - n),
                        mode='constant')
        fft_features = np.abs(np.fft.fft(ts_padded))[:5]
        fft_features = np.abs(np.fft.fft(ts))[:5]  # å–å‰5å€‹é »ç‡åˆ†é‡
        features.extend(fft_features)
        
        # è‡ªç›¸é—œç‰¹å¾µ
        acf = np.correlate(ts, ts, mode='full') / len(ts)
        features.extend(acf[len(acf)//2:len(acf)//2+3])  # å–ä¸­å¿ƒé»å¾Œ3å€‹å€¼
    
    # ä¸‰ç¶­å‘é‡ç‰¹å¾µ
    speed_magnitudes = np.linalg.norm(speeds, axis=1)
    pos_magnitudes = np.linalg.norm(positions, axis=1)
    
    features.extend([
        np.mean(speed_magnitudes),
        np.std(speed_magnitudes),
        np.max(speed_magnitudes),
        np.min(speed_magnitudes),
        np.mean(pos_magnitudes),
        np.std(pos_magnitudes),
        np.max(pos_magnitudes),
        np.min(pos_magnitudes)
    ])
    
    # --- æŠŠ features è½‰æˆ 1-D list ---
    flat = []
    for f in features:
        if isinstance(f, np.ndarray):
            flat.extend(f.ravel())        # array(3,) â†’ 3 å€‹å€¼
        else:                             # Python float / int / scalar
            flat.append(float(f))

    flat = np.asarray(flat, dtype=np.float32)

    # ---- ä¿è­‰é•·åº¦ä¸€è‡´ï¼šä¸è¶³è£œ 0ï¼Œå¤šé¤˜æˆªæ–· ----
    if flat.size < EXPECTED_DIM:
        flat = np.pad(flat, (0, EXPECTED_DIM - flat.size))
    elif flat.size > EXPECTED_DIM:
        flat = flat[:EXPECTED_DIM]

    return flat



def generate_features(raw_dir: str, info_csv:str,  out_dir: str):
    # è®€å– cut_pointï¼›ç”¨ unique_id ç•¶ index æ–¹ä¾¿éš¨æŸ¥
    info_df = pd.read_csv(info_csv).set_index("unique_id")

    Path(out_dir).mkdir(exist_ok=True)
    pathlist_txt = Path(raw_dir).glob('*.txt')

    for file in pathlist_txt:
        # è®€å–æ–‡ä»¶
        data = []
        with open(file, 'r') as f:
            for line in f.readlines()[1:]:  # Skip header
                if line.strip():  # Skip empty lines
                    values = line.strip().split()
                    if len(values) >= 6:
                        # åªå–å‰6å€‹å€¼ï¼Œä¸¦è½‰æ›ç‚ºæ•´æ•¸
                        row = [int(x) for x in values[:6]]
                        data.append(row)

        if not data:
            print(f"Warning: No valid data found in {file}")
            logging.warning(f"No valid data found in {file}")
            continue

        # å‰µå»ºDataFrameä¸¦å‘½ååˆ—
        df = pd.DataFrame(data, columns=[
            'Ax', 'Ay', 'Az',    # Accelerometer
            'Gx', 'Gy', 'Gz'     # Gyroscope
        ])
        
        try:
            # --- ä¾ cut_point ç²¾æº–åˆ‡ 27 æ®µï¼›ç¼ºåˆ‡é»å‰‡å›é€€ç­‰åˆ† ---
            uid        = int(file.stem)
            try:
               cuts_raw = info_df.loc[uid, "cut_point"]          # "[0 35 ... 998]"
               cuts     = np.fromstring(cuts_raw.strip("[]"), sep=" ", dtype=int)
               assert len(cuts) == 28
               segments  = [df.iloc[cuts[i]:cuts[i+1]] for i in range(27)]
            except Exception as _:
               # è‹¥è©²æª”æ¡ˆæ²’æœ‰ cut_point æˆ–è§£æå¤±æ•— â†’ å›é€€å‡åˆ†
               idx_splits = np.array_split(np.arange(len(df)), 27)
               segments   = [df.iloc[idx] for idx in idx_splits]
            seg_feats = [extract_features(seg) for seg in segments]

            # ğŸŸ¢ ç¢ºèªæ‰€æœ‰æ®µè½å‘é‡é•·åº¦ä¸€è‡´ï¼Œå¦å‰‡æ¨æ£„ç•°å¸¸æ®µ
            base_len = len(seg_feats[0])
            seg_feats = [v for v in seg_feats if len(v) == base_len]
            if len(seg_feats) == 0:                 # å…¨éƒ¨å¤±æ•— â†’ å¡« 0
                seg_feats = [np.zeros(base_len)]
            # é€™è£¡ç¤ºç¯„å–å¹³å‡ï¼›æƒ³å–æœ€å¤§å€¼å¯æ”¹ np.max(seg_feats, axis=0)
            features  = np.mean(seg_feats, axis=0)
            
            # ä¿å­˜ç‰¹å¾µ
            features_df = pd.DataFrame([features])
            output_path = Path(out_dir) / f"{file.stem}.csv"
            features_df.to_csv(output_path, index=False)
        except Exception as e:
            print(f"Error processing {file}: {str(e)}")
            logging.error(f"Error processing {file}: {str(e)}")
            continue

def evaluate_predictions(pred_file: str, info_file: str):
    """Evaluate predictions against true values"""
    pred = pd.read_csv(pred_file)
    info = pd.read_csv(info_file)

    # Filter info to include only rows matching unique_id in pred
    info = info[info['unique_id'].isin(pred['unique_id'])]

    # Sort and align data by unique_id
    pred = pred.sort_values('unique_id').reset_index(drop=True)
    info = info.sort_values('unique_id').reset_index(drop=True)

    # Ensure alignment of unique_ids
    if not np.array_equal(pred['unique_id'].values, info['unique_id'].values):
        raise ValueError("Prediction and info DataFrames have mismatched unique_ids")

    # Binary tasks
    gender_true = (info["gender"] == 1).astype(int)
    hold_true = (info["hold racket handed"] == 1).astype(int)

    gender_auc = roc_auc_score(gender_true, pred["gender"])
    hold_auc = roc_auc_score(hold_true, pred["hold racket handed"])

    # Multiclass tasks - play years
    play_years_cols = [f"play years_{i}" for i in range(3)]
    play_year_true = pd.get_dummies(info["play years"]).reindex(columns=range(3), fill_value=0)
    play_year_pred = pred[play_years_cols]
    play_year_auc = roc_auc_score(
        play_year_true, play_year_pred,
        multi_class="ovr", average="micro"
    )

    # Multiclass tasks - level
    level_cols = [f"level_{i}" for i in [2, 3, 4, 5]]
    level_true = pd.get_dummies(info["level"]).reindex(columns=[2, 3, 4, 5], fill_value=0)
    level_pred = pred[level_cols]
    level_auc = roc_auc_score(
        level_true, level_pred,
        multi_class="ovr", average="micro"
    )

    final_score = (gender_auc + hold_auc + play_year_auc + level_auc) / 4
    print(f"Gender ROC AUC       : {gender_auc:.4f}")
    print(f"Hold Racket ROC AUC  : {hold_auc:.4f}")
    print(f"Play Years ROC AUC   : {play_year_auc:.4f}")
    print(f"Level ROC AUC        : {level_auc:.4f}")
    print(f"Final Score          : {final_score:.4f}")
    return {
        'gender': gender_auc,
        'hold': hold_auc,
        'play_years': play_year_auc,
        'level': level_auc,
        'final': final_score
    }
    
if __name__ == '__main__':
    evaluate_predictions("val_pred.csv", "train_info.csv")
